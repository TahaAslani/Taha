<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Taha ValizadehAslani</title>
  <meta name="description" content="Personal site for Taha ValizadehAslani" />
  <meta name="color-scheme" content="light dark" />
  <link rel="icon" href="/favicon.ico" />
  <style>
    :root {
      --bg: #ffffff;
      --ink: #151515;
      --ink-2: #545454;
      --rule: #e4e4e4;
      --accent: #1a4dd6;
      --col-rail: 300px;
      --col-gap: 56px;
      --measure: 68ch;
      --leading: 1.55;
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #0f1113;
        --ink: #e9eaec;
        --ink-2: #a1a6ad;
        --rule: #2a2e33;
        --accent: #84a7ff;
      }
    }
    html { -webkit-text-size-adjust: 100%; }
    * { box-sizing: border-box; }
    body {
      margin: 0;
      background: var(--bg);
      color: var(--ink);
      font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
      line-height: var(--leading);
    }
    .wrap { display: grid; grid-template-columns: var(--col-rail) var(--col-gap) 1fr; min-height: 100dvh; }
    .rail { padding: 48px 24px 48px 56px; border-right: 1px solid var(--rule); position: sticky; top: 0; align-self: start; height: 100dvh; }
    .main { padding: 64px 56px 96px; max-width: 1200px; }
    .name { font-family: ui-serif, Georgia, "Times New Roman", serif; font-weight: 700; font-size: 24px; margin: 0 0 4px; }
    .role { color: var(--ink-2); margin: 0 0 28px; font-size: 14px; }
    nav a { display: block; text-decoration: none; color: var(--ink-2); padding: 6px 0; font-weight: 600; }
    nav a:hover, nav a[aria-current="page"] { color: var(--ink); text-decoration: underline; text-underline-offset: 3px; }
    h1, h2, h3 { font-weight: 800; margin: 0; line-height: 1.2; }
    h1 { font-size: 40px; margin-bottom: 12px; }
    h2 { font-size: 24px; margin: 48px 0 12px; border-top: 1px solid var(--rule); padding-top: 24px; }
    h3 { font-size: 18px; margin: 28px 0 6px; }
    p { margin: 0 0 14px; max-width: var(--measure); color: var(--ink); }
    .muted { color: var(--ink-2); }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; text-underline-offset: 2px; }
    ul { padding-left: 18px; margin: 0 0 14px; max-width: var(--measure); }
    .intro { max-width: var(--measure); }
    .meta { display: flex; gap: 14px; color: var(--ink-2); font-size: 14px; }
    .projects { display: grid; grid-template-columns: 1fr; gap: 40px; max-width: 1000px; }
    .project { border-top: 1px solid var(--rule); padding-top: 16px; }
    .project img {
    display: block;
    max-width: 68ch;  /* same as paragraph text width */
    width: 100%;
    height: auto;
    margin: 12px auto;
    }
    .contact dd, .contact dt { margin: 0; }
    .contact dt { color: var(--ink-2); font-size: 12px; text-transform: uppercase; letter-spacing: .08em; margin-top: 18px; }
    footer { border-top: 1px solid var(--rule); margin-top: 64px; padding-top: 16px; color: var(--ink-2); font-size: 14px; }
    @media (max-width: 1100px) {
      .wrap { grid-template-columns: 1fr; }
      .rail { position: static; height: auto; border-right: none; border-bottom: 1px solid var(--rule); padding: 28px 24px; }
      .main { padding: 40px 24px 64px; }



    }
  </style>
</head>
<body>
  <div class="wrap">
    <aside class="rail">
      <h1 class="name">Taha ValizadehAslani</h1>
      <p class="role">Machine Learning Engineer <br> Philadelphia, USA</p>
<!--       <p class="role">Philadelphia, USA</p> -->
      <nav>
        <a href="#about" aria-current="page">About</a>
        <a href="#work">Selected Work</a>
        <a href="#Education">Education</a>
        <a href="#Publications">Publications</a>
        <a href="#contact">Contact</a>
        <a href="https://drive.google.com/file/d/1uOUNj2PBIcHeJd35O1pCTRXo033M0xyq/view?usp=sharing">Résumé</a>
      </nav>
    </aside>
    <div></div>
    <main class="main">
      <section id="about" class="intro">
        <h1>Theory in Service of Practice</h1>
        <p class="muted">I work on <strong>Machine learning</strong>, <strong>Natural language processing</strong>, <strong>Data science</strong>,  and <strong>Information Theory</strong>. I care about models that not only get the job done, but also are theoretically proven to be optimal</p>
        <div class="meta" style="margin-top:8px">
          <span><a href="https://drive.google.com/file/d/1uOUNj2PBIcHeJd35O1pCTRXo033M0xyq/view?usp=sharing">Résumé</a></span>
          <span>•</span>
          <span><a href="mailto:taha.valizadeh@gmail.com">Email</a></span>
          <span>•</span>
          <span><a href="https://scholar.google.com/citations?user=WxaHx0sAAAAJ&hl=en">Google Scholar</a></span>
          <span>•</span>
          <span><a href="https://github.com/TahaAslani" target="_blank" rel="noreferrer noopener">GitHub</a></span>
          <span>•</span>
          <span><a href="https://www.linkedin.com/in/taha-valizadehaslani-699538124" target="_blank" rel="noreferrer noopener">LinkedIn</a></span>
        </div>
      </section>


      

      
      <section id="work">
        <h2>Selected Work</h2>
        <div class="projects">

          <article class="project">
            <h3>
              <a href="https://academic.oup.com/bib/article/24/4/bbad226/7197744" target="_blank" rel="noreferrer noopener">
                PharmBERT: An LLM for pharmaceutical texts
              </a>
            </h3>
            <p class="muted">Pharmaceutical text found on drug labels possesses distinct features that differentiate it significantly from the regular text. This specificity stems from the specialized language, terminology, and regulatory requirements that shape the drug label domain. As a consequence, a generic large language model (LLM), often known as a vanilla model might not exhibit optimal effectiveness when applied to the drug label text. This is primarily because such generic models lack the domain-specific knowledge required to accurately interpret and process the nuances and specialized idiosyncrasies inherent in pharmaceutical texts.</p>
            <img src="images/PharmBERT.png" alt="PharmBERT concept art illustrating domain-specific pretraining for drug labels" style="max-width:60%; height:auto; margin:8px 0;">
            <p class="muted">This is where PharmBERT comes into play. Developed for the US Food and Drug Administration (FDA), PharmBERT is not just another addition to the family of BERT models; it is a purpose-built, specialized model that has been meticulously trained to grasp and interpret the unique aspects of pharmaceutical text, particularly drug labels. This model is designed to bridge the gap between general language processing capabilities and the specific demands of the drug label domain. By focusing on the peculiarities of pharmaceutical language, PharmBERT aims to provide more accurate, reliable, and contextually relevant insights from drug label texts than what would be achievable with a general-purpose model. In creating PharmBERT, we have not only leveraged the powerful architecture of BERT models but also enriched it with a training regime that includes a vast and diverse range of pharmaceutical texts. This specialized training ensures that PharmBERT is adept at handling the kind of linguistic challenges that are typical in drug labels, such as complex compound names, medical terminology, and regulatory language. The outcome is a model that is finely tuned to deliver enhanced performance in the analysis of drug label text, thereby offering a valuable tool for pharmaceutical companies, healthcare professionals, and regulatory bodies in their endeavors to understand and utilize the wealth of information contained in drug labels.</p>
          </article>

          <article class="project">
              <h3>
                <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7694136/" target="_blank" rel="noreferrer noopener">
                  Amino Acid k-mer Feature Extraction for Quantitative Antimicrobial Resistance (AMR) Prediction
                </a>
              </h3>
            <p class="muted">Microbes can adapt over time, developing resistance to antibiotics—a phenomenon known as antimicrobial resistance (AMR). Once this resistance emerges, treating infections becomes much more difficult, often requiring stronger or alternative antibiotics, many of which may carry greater risks. Misuse and overuse of antibiotics have accelerated this process, putting constant evolutionary pressure on microbes and driving the rise of AMR. Today, this issue is widely regarded as one of the most serious public-health threats, with the potential to trigger crises comparable to pandemics if effective treatments are unavailable.</p>
            <p class="muted">The aim of this project was to design and test machine learning approaches that leverage bacterial genome sequences in two ways: first, to estimate the minimum antibiotic dosage needed to combat an infection; and second, to pinpoint mutations or genes that contribute to resistance. I introduced a new feature extraction method, counting Amino-Acid k-mers, enables machine learning models to recognize recurring patterns in amino acid sequences within bacterial genes. I compared the proposed method to the existing methods, such as counting nucleotide (NT) k-mers, gene searching, and Single-nucleotide polymorphism (SNP) calling. The proposed approach not only matches or surpasses the accuracy of current techniques for predicting dosage but also offers researchers deeper insight into how resistance arises at the molecular level.</p>
            <img src="images/AMR.png" alt="Visualization of implemented methods for analyzing bacterial genomes" style="max-width:60%; height:auto; margin:8px 0;">
          </article>
          
          <article class="project">
            <h3>
              <a href="https://www.sciencedirect.com/science/article/pii/S0925231224005721" target="_blank" rel="noreferrer noopener">
                Two-stage fine-tuning for learning class-imbalanced data
              </a>
            </h3>  
            <p class="muted">In practical classification scenarios, it's common to encounter an imbalanced or long tailed distribution of classes. This situation arises when certain classes within the dataset, known as minority classes, are represented by a relatively small number of samples, while others, referred to as majority classes, are characterized by a significantly larger number of samples. This non-uniform distribution of samples among classes presents a challenge to a wide array of machine learning algorithms. This issue is further exacerbated if the selected performance metric values all classes equally, irrespective of their frequency in the dataset, for reasons such as fairness and inclusivity or because of an intrinsic interest in the minority class.</p>
            <p class="muted">To cope with these challenges, a simple modification of standard fine-tuning is employed: Two-stage fine-tuning. In Stage 1, the final layer of the pretrained model is pre-finetuned. The pre-finetuning can be done either by training the last layer with class-balanced augmented data, generated using ChatGPT, or with a class-balanced reweighting method used with the original data. In Stage 2, the standard fine-tuning is performed. This modification has several benefits: (1) it leverages pretrained representations by only finetuning a small portion of the model parameters while keeping the rest untouched; (2) it allows the model to learn an initial representation of the specific task; and importantly (3) it protects the learning of tail classes from being at a disadvantage during the model updating. The experimental results show that the proposed two-stage fine-tuning outperforms vanilla fine-tuning and state-of-the-art methods on different datasets.</p>
            <img src="images/two-stage.jpg" alt="Two-stage fine-tuning" style="max-width:60%; height:auto; margin:8px 0;">
          </article>

          <article class="project">
            <h3>
              <a href="https://www.sciencedirect.com/science/article/pii/S1874490718302350" target="_blank" rel="noreferrer noopener">
                Mathematical analysis and improvement of IS-LDPC codes
              </a>
            </h3>  
            <p class="muted">Peak-to-Average Power Ratio (PAPR) is a major shortcoming of Orthogonal Frequency Division Multiplexing (OFDM) systems. One promising solution to this issue is the use of Invertible-Subset Low-Density Parity-Check (IS-LDPC) codes. Although IS-LDPC codes are particularly effective at controlling PAPR with low search complexity, their error-control performance degrades as the number of invertible subsets increases.</p>
            <p class="muted">To investigate the reasons for this performance degradation, I conducted a mathematical analysis of the code construction and proved four theorems that determine the probability of key events in construction of such codes, while also establishing bounds on the decisions that influence their performance. Based on these theorems, I proposed a heuristic search method designed to enhance the error-control performance of IS-LDPC codes, while maintaining their favorable PAPR control characteristics and low search complexity. Computer simulations show that the proposed method decreases the probability of bit error rate (BER) and frame error rate (FER) across different configurations of IS-LDPC codes.</p>
            <img src="images/IS-LDPC.png" alt="Mathematical analysis of construction of IS-LDPC codes" style="max-width:60%; height:auto; margin:8px 0;">
          </article>
        </div>
      </section>



      <section id="Education">
        <h2>Education</h2>
        <ul class="edu-list">
          <li>
            <strong>Drexel University</strong> — Ph.D. in Electrical Engineering<br>
            <span class="muted">Philadelphia, PA • Sept 2018 – Mar 2024</span>
          </li>
          <li>
            <strong>Iran University of Science and Technology</strong> — M.Sc. in Electrical Engineering<br>
            <span class="muted">Tehran, Iran • Sept 2014 – Jan 2017</span>
          </li>
          <li>
            <strong>Lorestan University</strong> — B.Sc. in Electrical Engineering (Power)<br>
            <span class="muted">Khorramabad, Iran • Feb 2009 – May 2013</span>
          </li>
        </ul>
      </section>

      

      <section id="Publications">
        <h2>Publications</h2>
      
        <h3>Journal and Conference Papers</h3>
        <ul class="pub-list">
          <li>
            <a href="https://doi.org/10.1016/j.neucom.2024.127801">
              <strong>Taha ValizadehAslani</strong>, Yiwen Shi, Jing Wang, Ping Ren, Yi Zhang, Meng Hu, Liang Zhao, Hualou Liang.
              “Two-Stage Fine-Tuning with ChatGPT Data Augmentation for Learning Class-Imbalanced Data.”
              <em>Neurocomputing</em>, 127801. 2024.
            </a>
          </li>
      
          <li>
            <a href="https://arxiv.org/abs/2403.20284">
              <strong>Taha ValizadehAslani</strong>, Hualou Liang.
              “LayerNorm: A key component in parameter-efficient fine-tuning.” arXiv:2403.20284.
            </a>
          </li>
      
          <li>
            <a href="https://academic.oup.com/bib/advance-article-abstract/doi/10.1093/bib/bbad226/7197744?redirectedFrom=fulltext">
              <strong>Taha ValizadehAslani</strong>, Yiwen Shi, Ping Ren, Jing Wang, Yi Zhang, Meng Hu, Liang Zhao, Hualou Liang.
              “PharmBERT: a domain-specific BERT model for drug labels.”
              <em>Briefings in Bioinformatics</em> 24(4), bbad226. 2023.
            </a>
          </li>
      
          <li>
            <a href="https://doi.org/10.1016/j.jbi.2023.104533">
              Yiwen Shi, Ping Ren, Jing Wang, Biao Han, <strong>Taha ValizadehAslani</strong>, Felix Agbavor, Yi Zhang, Meng Hu, Liang Zhao, Hualou Liang.
              “Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting.”
              <em>Journal of Biomedical Informatics</em> 148, 104533. 2023.
            </a>
          </li>
      
          <li>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S1532046423000060?CMX_ID=&SIS_ID=&dgcid=STMJ_AUTH_SERV_PUBLISHED&utm_acid=88471714&utm_campaign=STMJ_AUTH_SERV_PUBLISHED&utm_in=DM331526&utm_medium=email&utm_source=AC_">
              Yiwen Shi, Jing Wang, Ping Ren, <strong>Taha ValizadehAslani</strong>, Yi Zhang, Meng Hu, Hualou Liang.
              “Fine-Tuning BERT for Automatic ADME Semantic Labeling in FDA Drug Labeling to Enhance Product-Specific Guidance Assessment.”
              <em>Journal of Biomedical Informatics</em> 138, 104285. 2023.
            </a>
          </li>
      
          <li>
            <a href="https://proceedings.mlr.press/v183/shi22a.html">
              Yiwen Shi, <strong>Taha ValizadehAslani</strong>, Jing Wang, Ping Ren, Yi Zhang, Meng Hu, Liang Zh, Hualou Liang.
              “Improving Imbalanced Learning by Pre-finetuning with Data Augmentation.”
              <em>Fourth International Workshop on Learning with Imbalanced Domains: Theory and Applications</em>. 2022.
            </a>
          </li>
      
          <li>
            <a href="https://www.frontiersin.org/articles/10.3389/fgene.2021.628758/full">
              Waleed Iqbal, Elena V. Demidova, Samantha Serrao, <strong>Taha ValizadehAslani</strong>, Gail Rosen, Sanjeevani Arora.
              “RRM2B Is Frequently Amplified Across Multiple Tumor Types: Implications for DNA Repair, Cellular Survival, and Cancer Therapy.”
              <em>Frontiers in Genetics</em> 12, 628758. 2021.
            </a>
          </li>
      
          <li>
            <a href="https://www.mdpi.com/2079-7737/9/11/365">
              <strong>Taha ValizadehAslani</strong>, Zhengqiao Zhao, Bahrad A Sokhansanj, Gail L Rosen.
              “Amino Acid k-mer Feature Extraction for Quantitative Antimicrobial Resistance (AMR) Prediction by Machine Learning and Model Interpretation for Biological Insights.”
              <em>Biology</em> 9(11), 365. 2020.
            </a>
          </li>
      
          <li>
            <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7583716/">
              Chiahui Chen, <strong>Taha ValizadehAslani</strong>, Gail Rosen, Carla Jungquist, Laura Anderson.
              “Healthcare Shift Workers’ Temporal Habits for Eating, Sleeping, and Light Exposure: A Multi-Instrument Pilot Study.”
              <em>Journal of Circadian Rhythms</em> 18. 2020.
            </a>
          </li>
      
          <li>
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S1874490718302350">
              <strong>Taha ValizadehAslani</strong>, Abolfazl Falahati.
              “An analysis and improvement of error control performance of IS-LDPC codes with a large number of subsets.”
              <em>Physical Communication</em> 31, 79–86. 2018.
            </a>
          </li>
        </ul>
      
        <h3>Academic Peer Review</h3>
        <ul class="pub-list">
          <li>Reviewed for <em>PLOS Digital Health</em>.</li>
        </ul>
      </section>
            
      
      
      <section id="contact">
        <h2>Contact</h2>
        <dl class="contact">
          <dt>Email</dt>
          <dd><a href="mailto:taha.valizadeh@gmail.com">taha.valizadeh@gmail.com</a></dd>
        </dl>
        <footer>
          © <span id="y"></span> Taha ValizadehAslani
        </footer>
      </section>
    </main>
  </div>
  <script>
    document.getElementById('y').textContent = new Date().getFullYear();
  </script>
</body>
</html>


